{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "reg_lmbd = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='data/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "test_dataset = dsets.MNIST(root='data/', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContractiveAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContractiveAutoencoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784,400,bias=False),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(400,784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(x)\n",
    "        return x,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation reference - https://wiseodd.github.io/techblog/2016/12/05/contractive-autoencoder/\n",
    "\n",
    "class ContractiveAELoss(nn.Module):\n",
    "    def __init__(self,lmbd):\n",
    "        super(ContractiveAELoss,self).__init__()\n",
    "        self.bceloss = nn.BCELoss()\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "    def forward(self,targets,actual,weights,hidden_op):\n",
    "        loss_bce  = self.bceloss(targets,actual)\n",
    "        dh = hidden_op * (1-hidden_op)\n",
    "        w_sum = torch.sum(weights ** 2,dim=1)\n",
    "        #print(w_sum.size())\n",
    "        w_sum = w_sum.unsqueeze(1)\n",
    "        #print(w_sum.size())\n",
    "        contractive_loss = torch.sum(torch.mm(dh**2, w_sum), 0)\n",
    "        total_loss = loss_bce + contractive_loss.mul_(self.lmbd)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContractiveAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContractiveAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=False)\n",
       "    (1): ReLU(inplace)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=784, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.0.weight',\n",
       "              tensor([[ 2.0781e-02,  3.3058e-03, -7.4142e-03,  ..., -2.8297e-02,\n",
       "                       -2.8479e-02,  2.1977e-02],\n",
       "                      [ 2.7651e-02, -1.9840e-02, -1.8380e-02,  ..., -8.3144e-03,\n",
       "                        1.9902e-02,  3.5028e-02],\n",
       "                      [-2.8927e-02,  2.8389e-02, -1.1021e-02,  ...,  2.6231e-03,\n",
       "                       -1.9823e-02,  6.6733e-03],\n",
       "                      ...,\n",
       "                      [-8.4967e-03,  2.6137e-02,  2.1342e-02,  ..., -6.1755e-03,\n",
       "                       -3.3788e-02,  2.3377e-03],\n",
       "                      [ 1.3155e-02, -1.8176e-02,  2.9448e-02,  ...,  7.0226e-03,\n",
       "                       -2.7186e-02, -8.6945e-03],\n",
       "                      [-5.1170e-03, -2.8926e-02, -2.8630e-02,  ...,  3.0332e-02,\n",
       "                       -2.6075e-02,  3.1407e-02]])),\n",
       "             ('decoder.0.weight',\n",
       "              tensor([[ 8.1783e-03, -3.9050e-02,  4.0053e-02,  ..., -2.4830e-02,\n",
       "                        2.4144e-02,  1.5522e-02],\n",
       "                      [-2.6041e-02,  2.8727e-02, -3.0050e-02,  ..., -3.2475e-02,\n",
       "                        2.1277e-02,  1.0509e-02],\n",
       "                      [ 1.8328e-02, -1.1706e-02, -1.7656e-02,  ..., -2.6980e-02,\n",
       "                        3.1664e-02, -1.2261e-02],\n",
       "                      ...,\n",
       "                      [ 3.2698e-02,  1.2478e-02,  2.6988e-02,  ...,  1.5324e-02,\n",
       "                       -3.1566e-02,  4.2540e-02],\n",
       "                      [ 4.3755e-02, -2.1572e-02, -2.5854e-02,  ...,  4.7176e-03,\n",
       "                       -1.3335e-02, -4.1256e-02],\n",
       "                      [ 3.2735e-02, -8.4265e-03, -2.2008e-02,  ...,  2.1481e-02,\n",
       "                        2.6106e-02, -3.0831e-02]])),\n",
       "             ('decoder.0.bias', tensor(1.00000e-02 *\n",
       "                     [-1.0083, -3.9317,  0.2557,  0.3920,  2.2487,  1.4700,  4.0143,\n",
       "                       3.3932, -3.4432,  0.6265, -0.5282, -4.3990, -0.3342,  4.9858,\n",
       "                       4.4262, -1.1111,  1.8662, -1.0593, -2.1266,  4.2859,  2.2170,\n",
       "                       4.4958, -4.0637,  3.0518,  0.2744,  0.2818,  1.4886, -2.0486,\n",
       "                      -3.4508,  2.0344,  0.1503,  0.7210, -3.0395,  0.2310, -4.6404,\n",
       "                      -1.9039,  2.5890, -3.1069,  1.3754, -2.3610,  3.0355, -1.4923,\n",
       "                       1.9571, -1.5688, -4.0701, -4.6965,  0.5222,  0.7793,  0.4601,\n",
       "                      -1.1016,  4.7648,  2.9194,  1.1743, -0.4443,  4.7791, -3.9542,\n",
       "                       1.1845,  4.8068, -4.6762, -1.8970,  0.1518,  3.1513, -4.8258,\n",
       "                       2.9248, -4.1445,  2.6458, -4.1995, -3.2518,  0.9289, -0.1842,\n",
       "                       3.7649,  1.1428, -2.7383, -0.0912, -1.0617,  4.3003,  1.3409,\n",
       "                      -2.2696, -2.4271,  3.5897, -2.4899, -2.7977,  1.6468, -0.5141,\n",
       "                      -1.2757, -1.7873, -4.4098, -2.8333, -1.9745,  1.4182, -4.5483,\n",
       "                      -3.9172, -1.6031,  2.9560,  3.2361, -0.6819, -1.1584,  0.4629,\n",
       "                       3.9986, -1.5245, -0.3102, -4.2760, -0.9962, -1.4269, -1.3802,\n",
       "                       1.7001, -3.0144,  2.4250, -3.0333,  2.7107,  3.1716,  3.4783,\n",
       "                      -2.8873,  1.7448, -1.6993,  0.2620, -2.5479, -0.2456, -2.7196,\n",
       "                       2.9726,  1.1480, -1.1557,  0.0089,  2.8217, -3.0428, -0.2618,\n",
       "                      -1.5379, -0.1669,  2.6927, -2.9643, -4.9985, -1.3279, -2.5078,\n",
       "                       1.3804, -3.2603,  0.7959,  3.9838, -0.5004,  3.4374, -3.4200,\n",
       "                       4.1172, -3.4338,  2.9616,  0.1004,  1.9999,  4.2215, -4.2416,\n",
       "                      -0.4030, -0.0102,  0.0345, -2.9129, -3.9749,  1.8740,  4.5973,\n",
       "                       4.5578,  2.6617, -4.6046, -0.0501,  0.7517,  1.6476,  4.9469,\n",
       "                      -4.4578,  1.2836,  3.4744,  1.3803, -1.8760, -0.9577,  0.8042,\n",
       "                       0.7348, -2.3955, -1.7234, -1.9693,  2.5669,  4.0705,  4.2086,\n",
       "                      -2.2256,  4.0590, -4.5390,  0.0768,  0.2613,  1.7563, -0.0622,\n",
       "                       2.8392, -4.2486, -4.5009, -0.8256,  1.7427,  0.6589, -2.0733,\n",
       "                       1.4181,  2.2777,  0.8801, -2.8975, -2.8701,  3.9637,  2.8984,\n",
       "                       4.4712,  4.5980,  1.2899,  1.2282, -3.1118, -3.4435,  0.8842,\n",
       "                       1.7782,  3.2253,  4.8685, -3.2633, -2.0172,  2.0318,  3.8697,\n",
       "                       0.7079,  3.5485,  0.0614,  4.8644,  4.5123, -4.4521, -0.2676,\n",
       "                       1.2170,  4.5570, -4.1399, -1.9614, -1.1235, -0.5940, -4.6443,\n",
       "                      -2.0763, -0.0544,  0.9046, -2.3579, -4.7090, -0.0111,  0.9532,\n",
       "                      -4.4337,  2.6879,  0.2795, -4.2017,  1.6206,  0.6453,  4.4719,\n",
       "                      -1.9030,  0.0853,  1.3859,  1.1558,  0.7600, -2.0406, -0.5257,\n",
       "                       1.7165, -2.4916, -3.0010, -0.4867, -4.2582,  4.5543, -1.8554,\n",
       "                      -3.4843, -3.7342,  3.6464,  3.6531, -1.6000, -4.1566, -1.0311,\n",
       "                      -4.5756, -1.5567, -4.5286,  0.4128, -1.8175, -4.8729,  4.5863,\n",
       "                      -1.9701, -1.3670, -3.3200,  4.5465, -1.9924, -4.5853, -0.2561,\n",
       "                       0.4699,  3.3389, -1.1061,  2.0450,  4.9637, -3.2124,  2.1097,\n",
       "                      -0.1955,  0.2049,  0.9024, -0.7663,  1.2881, -2.7255, -4.8264,\n",
       "                      -1.3055,  4.7832,  0.6996,  1.4994, -2.8277,  3.6165,  0.5369,\n",
       "                       1.4629,  3.4419,  3.6718, -1.0561,  0.1136, -3.5600, -3.0482,\n",
       "                       2.5065,  2.7161,  4.1529, -2.7626,  4.2654,  2.1743, -4.4143,\n",
       "                      -1.5778,  2.7279,  1.9474, -1.1621,  1.5837,  2.9531, -0.8802,\n",
       "                      -3.8504, -1.5930,  0.0068,  1.1059,  0.3511,  1.8144,  4.5334,\n",
       "                       4.5836,  4.1313, -1.7637, -1.1667, -4.0554, -1.4322,  1.3558,\n",
       "                       1.2297,  4.6287, -1.9837, -3.5699,  3.1162, -0.3146,  1.2460,\n",
       "                      -3.3069, -2.4613, -1.5590, -2.4580,  1.3127,  1.4818,  3.6374,\n",
       "                       3.6022, -1.6448,  3.0750,  0.6909,  1.9514, -0.3491,  4.0199,\n",
       "                      -0.6599, -1.8092, -1.9240,  4.9832,  2.0790, -1.4708, -4.8356,\n",
       "                       3.3986,  0.9269, -4.0608,  1.4753, -4.0022, -0.7924, -2.6038,\n",
       "                       2.7695,  3.0964,  2.9150,  2.5634,  1.4014, -3.0399, -0.7019,\n",
       "                      -3.8946,  0.0011,  4.4577, -4.3042,  4.0910,  4.6474, -2.2145,\n",
       "                      -2.2518,  2.3574,  3.4427,  1.4133,  2.9290, -1.3247,  2.1147,\n",
       "                      -0.1328,  0.7164,  1.6734,  4.3593,  4.0419, -0.1176,  4.9212,\n",
       "                      -1.8723,  4.3591, -0.6124, -2.5120,  3.1660,  1.8275, -4.5167,\n",
       "                       2.0965, -3.0763,  3.2377, -1.2412, -3.7698, -0.9780,  1.3002,\n",
       "                      -2.0788,  1.0347,  0.2715, -1.4899, -1.1849, -2.3050,  0.4563,\n",
       "                       4.0272,  3.3413,  1.9829,  1.0398, -1.5817, -3.9458,  2.1198,\n",
       "                       4.1891,  0.3380, -1.4786, -4.9658,  0.7713, -2.5692, -2.5885,\n",
       "                       0.0710, -1.9378,  4.0951, -1.1705, -3.3744, -0.2606, -1.8435,\n",
       "                      -3.7187, -4.5474,  3.3029, -0.9185, -3.6120, -0.8068,  0.7663,\n",
       "                       0.9855, -1.9810,  4.0273, -4.9116,  0.3771, -2.4991,  4.6040,\n",
       "                       1.5606,  3.7739,  3.3327,  4.5401, -0.7720, -3.6259, -3.9316,\n",
       "                       3.7047,  3.5278, -2.4292,  0.5001,  4.2655,  2.1370, -4.5382,\n",
       "                      -0.0433,  4.8646,  4.4365, -1.6134, -4.6927,  1.4049, -0.8431,\n",
       "                      -1.8910,  2.2862,  2.2024, -0.6674, -0.2308,  0.3772, -2.5898,\n",
       "                       0.5776, -3.9522,  4.3151, -3.7154,  1.2964, -4.6326, -4.1525,\n",
       "                       1.3913,  0.9557,  1.3256,  4.1144,  4.0616,  0.0554, -1.2139,\n",
       "                       2.6150,  2.2423,  4.7253,  2.1361,  3.9460,  1.7167, -0.9549,\n",
       "                      -1.0760, -3.5330,  3.8364, -3.0614, -2.5934,  3.9387, -0.0378,\n",
       "                      -1.2741, -1.8074, -3.1798, -4.3434,  0.0312,  4.4100, -1.6368,\n",
       "                      -2.1067, -3.8276,  3.5003, -4.8890,  1.7853,  4.3786,  1.4803,\n",
       "                       2.7386, -1.6547,  2.2398, -1.0898, -4.9751,  4.9041, -0.0310,\n",
       "                      -2.0836, -4.7051, -4.0967, -2.7909, -2.7400,  0.9261, -1.0562,\n",
       "                       0.0303,  1.5525, -1.5641, -3.4677,  0.9175,  1.2558,  2.6114,\n",
       "                      -4.4506, -3.3430,  4.3121, -4.2587, -3.5754,  0.0681, -4.3511,\n",
       "                       0.6878, -2.6785, -2.7110,  3.4123, -4.6983,  4.1970,  1.2354,\n",
       "                      -1.5412,  1.3803, -0.0360,  4.9336,  4.7106, -2.3455, -1.9097,\n",
       "                       4.8152,  3.7907, -4.1373,  1.8541, -2.7444, -0.5067,  4.2778,\n",
       "                      -1.8851,  3.6351,  2.3088, -3.5339, -0.9920,  4.9523, -1.8667,\n",
       "                      -0.2605,  3.2624,  4.7421, -3.8454, -2.0071, -4.6744, -2.7765,\n",
       "                      -0.1663, -2.5585, -0.4392, -0.0361, -2.9043,  1.2276,  2.4999,\n",
       "                       3.1523,  1.7722, -1.7332,  1.1323, -0.5097, -3.8889,  4.4389,\n",
       "                       3.0092,  0.5418, -3.5668, -3.8062, -2.7697,  3.1009,  4.5082,\n",
       "                       1.5390, -0.0434,  4.6864, -3.8667,  1.9230, -3.0349,  2.1713,\n",
       "                      -1.2739,  2.5274,  4.5906,  4.2624,  2.7948, -2.1924, -0.5829,\n",
       "                       1.4386,  0.0616,  4.5610, -1.6301,  1.3322, -0.1430,  2.5608,\n",
       "                      -0.2456, -2.3999, -4.0647, -1.3797, -1.7674, -1.8126, -4.3427,\n",
       "                      -3.4025,  1.7592,  0.9091,  3.0889,  4.9827, -1.5754,  4.7122,\n",
       "                       1.9241,  4.7486,  1.6170, -2.9291, -2.7152, -0.7122,  4.1379,\n",
       "                       2.0037, -3.0971, -0.4912,  0.7952,  1.1014,  1.5746,  3.0630,\n",
       "                      -4.4227, -1.9829, -3.8330,  2.4407, -4.5502, -1.9305,  3.3433,\n",
       "                       4.6336,  4.5198, -1.9299,  4.5168,  2.5437,  4.1251,  3.8859,\n",
       "                      -2.4620, -1.4077, -4.5877,  3.5594,  1.3417, -3.5275,  3.3321,\n",
       "                      -4.5053, -3.4290,  3.4419,  2.6293, -2.3526,  1.3979,  1.9407,\n",
       "                      -0.8796, -1.4699, -0.4266,  1.5914, -2.9351, -3.2976,  3.9655,\n",
       "                       1.7180,  3.1148,  4.9329, -4.2090, -3.7132,  4.8542,  0.2765,\n",
       "                       4.2391, -3.3171,  4.0094,  3.1999, -2.0106, -4.5193,  4.7665,\n",
       "                       4.5692,  2.4048,  0.3749, -3.2878,  3.4910,  1.7172,  3.6207,\n",
       "                      -4.5265,  3.5749,  2.8171,  0.6459,  2.8781, -2.5827,  3.0502,\n",
       "                      -2.0056, -1.7676, -3.8618, -2.9338,  4.1131, -0.2952, -4.6228,\n",
       "                       2.2592, -3.8258, -3.8817,  0.4156, -0.2108, -4.8553, -0.3400,\n",
       "                       1.7664, -2.6486,  2.6651,  3.7111, -3.3796,  3.0133, -0.3939,\n",
       "                       3.1042,  3.2473,  1.8039, -0.6343,  0.7404, -4.3688,  3.6188,\n",
       "                       0.2518,  3.0081,  4.8656, -0.1962,  1.5961, -4.7451, -1.5857,\n",
       "                       2.5434, -3.0832,  3.5469,  4.2667, -2.4875, -2.1852,  0.9178,\n",
       "                       1.6266,  3.3853,  2.8832,  2.6459, -0.8777,  4.6302, -3.5670,\n",
       "                      -4.6855,  3.6460,  0.4389,  2.7900, -2.9588, -0.9870,  3.8825,\n",
       "                      -3.3859,  4.9911, -2.6033, -4.1511, -4.9828, -1.2174,  2.8108,\n",
       "                       0.8978, -0.9974,  2.9433,  1.7889, -0.0325, -2.5014,  3.5415]))])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContractiveAELoss(reg_lmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/600], Loss: 0.2183\n",
      "Epoch [1/100], Step [200/600], Loss: 0.1838\n",
      "Epoch [1/100], Step [300/600], Loss: 0.1736\n",
      "Epoch [1/100], Step [400/600], Loss: 0.1595\n",
      "Epoch [1/100], Step [500/600], Loss: 0.1626\n",
      "Epoch [1/100], Step [600/600], Loss: 0.1520\n",
      "Epoch [2/100], Step [100/600], Loss: 0.1560\n",
      "Epoch [2/100], Step [200/600], Loss: 0.1470\n",
      "Epoch [2/100], Step [300/600], Loss: 0.1461\n",
      "Epoch [2/100], Step [400/600], Loss: 0.1468\n",
      "Epoch [2/100], Step [500/600], Loss: 0.1477\n",
      "Epoch [2/100], Step [600/600], Loss: 0.1424\n",
      "Epoch [3/100], Step [100/600], Loss: 0.1394\n",
      "Epoch [3/100], Step [200/600], Loss: 0.1386\n",
      "Epoch [3/100], Step [300/600], Loss: 0.1385\n",
      "Epoch [3/100], Step [400/600], Loss: 0.1419\n",
      "Epoch [3/100], Step [500/600], Loss: 0.1349\n",
      "Epoch [3/100], Step [600/600], Loss: 0.1352\n",
      "Epoch [4/100], Step [100/600], Loss: 0.1430\n",
      "Epoch [4/100], Step [200/600], Loss: 0.1344\n",
      "Epoch [4/100], Step [300/600], Loss: 0.1371\n",
      "Epoch [4/100], Step [400/600], Loss: 0.1337\n",
      "Epoch [4/100], Step [500/600], Loss: 0.1351\n",
      "Epoch [4/100], Step [600/600], Loss: 0.1326\n",
      "Epoch [5/100], Step [100/600], Loss: 0.1348\n",
      "Epoch [5/100], Step [200/600], Loss: 0.1343\n",
      "Epoch [5/100], Step [300/600], Loss: 0.1375\n",
      "Epoch [5/100], Step [400/600], Loss: 0.1292\n",
      "Epoch [5/100], Step [500/600], Loss: 0.1344\n",
      "Epoch [5/100], Step [600/600], Loss: 0.1339\n",
      "Epoch [6/100], Step [100/600], Loss: 0.1365\n",
      "Epoch [6/100], Step [200/600], Loss: 0.1373\n",
      "Epoch [6/100], Step [300/600], Loss: 0.1311\n",
      "Epoch [6/100], Step [400/600], Loss: 0.1265\n",
      "Epoch [6/100], Step [500/600], Loss: 0.1339\n",
      "Epoch [6/100], Step [600/600], Loss: 0.1333\n",
      "Epoch [7/100], Step [100/600], Loss: 0.1336\n",
      "Epoch [7/100], Step [200/600], Loss: 0.1315\n",
      "Epoch [7/100], Step [300/600], Loss: 0.1377\n",
      "Epoch [7/100], Step [400/600], Loss: 0.1332\n",
      "Epoch [7/100], Step [500/600], Loss: 0.1343\n",
      "Epoch [7/100], Step [600/600], Loss: 0.1290\n",
      "Epoch [8/100], Step [100/600], Loss: 0.1277\n",
      "Epoch [8/100], Step [200/600], Loss: 0.1274\n",
      "Epoch [8/100], Step [300/600], Loss: 0.1310\n",
      "Epoch [8/100], Step [400/600], Loss: 0.1318\n",
      "Epoch [8/100], Step [500/600], Loss: 0.1283\n",
      "Epoch [8/100], Step [600/600], Loss: 0.1301\n",
      "Epoch [9/100], Step [100/600], Loss: 0.1312\n",
      "Epoch [9/100], Step [200/600], Loss: 0.1298\n",
      "Epoch [9/100], Step [300/600], Loss: 0.1298\n",
      "Epoch [9/100], Step [400/600], Loss: 0.1280\n",
      "Epoch [9/100], Step [500/600], Loss: 0.1299\n",
      "Epoch [9/100], Step [600/600], Loss: 0.1249\n",
      "Epoch [10/100], Step [100/600], Loss: 0.1294\n",
      "Epoch [10/100], Step [200/600], Loss: 0.1271\n",
      "Epoch [10/100], Step [300/600], Loss: 0.1265\n",
      "Epoch [10/100], Step [400/600], Loss: 0.1280\n",
      "Epoch [10/100], Step [500/600], Loss: 0.1237\n",
      "Epoch [10/100], Step [600/600], Loss: 0.1237\n",
      "Epoch [11/100], Step [100/600], Loss: 0.1248\n",
      "Epoch [11/100], Step [200/600], Loss: 0.1225\n",
      "Epoch [11/100], Step [300/600], Loss: 0.1210\n",
      "Epoch [11/100], Step [400/600], Loss: 0.1232\n",
      "Epoch [11/100], Step [500/600], Loss: 0.1226\n",
      "Epoch [11/100], Step [600/600], Loss: 0.1247\n",
      "Epoch [12/100], Step [100/600], Loss: 0.1297\n",
      "Epoch [12/100], Step [200/600], Loss: 0.1281\n",
      "Epoch [12/100], Step [300/600], Loss: 0.1220\n",
      "Epoch [12/100], Step [400/600], Loss: 0.1218\n",
      "Epoch [12/100], Step [500/600], Loss: 0.1266\n",
      "Epoch [12/100], Step [600/600], Loss: 0.1232\n",
      "Epoch [13/100], Step [100/600], Loss: 0.1189\n",
      "Epoch [13/100], Step [200/600], Loss: 0.1257\n",
      "Epoch [13/100], Step [300/600], Loss: 0.1228\n",
      "Epoch [13/100], Step [400/600], Loss: 0.1233\n",
      "Epoch [13/100], Step [500/600], Loss: 0.1253\n",
      "Epoch [13/100], Step [600/600], Loss: 0.1177\n",
      "Epoch [14/100], Step [100/600], Loss: 0.1261\n",
      "Epoch [14/100], Step [200/600], Loss: 0.1259\n",
      "Epoch [14/100], Step [300/600], Loss: 0.1235\n",
      "Epoch [14/100], Step [400/600], Loss: 0.1257\n",
      "Epoch [14/100], Step [500/600], Loss: 0.1223\n",
      "Epoch [14/100], Step [600/600], Loss: 0.1238\n",
      "Epoch [15/100], Step [100/600], Loss: 0.1235\n",
      "Epoch [15/100], Step [200/600], Loss: 0.1256\n",
      "Epoch [15/100], Step [300/600], Loss: 0.1200\n",
      "Epoch [15/100], Step [400/600], Loss: 0.1201\n",
      "Epoch [15/100], Step [500/600], Loss: 0.1248\n",
      "Epoch [15/100], Step [600/600], Loss: 0.1235\n",
      "Epoch [16/100], Step [100/600], Loss: 0.1160\n",
      "Epoch [16/100], Step [200/600], Loss: 0.1236\n",
      "Epoch [16/100], Step [300/600], Loss: 0.1259\n",
      "Epoch [16/100], Step [400/600], Loss: 0.1237\n",
      "Epoch [16/100], Step [500/600], Loss: 0.1260\n",
      "Epoch [16/100], Step [600/600], Loss: 0.1228\n",
      "Epoch [17/100], Step [100/600], Loss: 0.1220\n",
      "Epoch [17/100], Step [200/600], Loss: 0.1208\n",
      "Epoch [17/100], Step [300/600], Loss: 0.1280\n",
      "Epoch [17/100], Step [400/600], Loss: 0.1285\n",
      "Epoch [17/100], Step [500/600], Loss: 0.1217\n",
      "Epoch [17/100], Step [600/600], Loss: 0.1250\n",
      "Epoch [18/100], Step [100/600], Loss: 0.1227\n",
      "Epoch [18/100], Step [200/600], Loss: 0.1262\n",
      "Epoch [18/100], Step [300/600], Loss: 0.1231\n",
      "Epoch [18/100], Step [400/600], Loss: 0.1275\n",
      "Epoch [18/100], Step [500/600], Loss: 0.1266\n",
      "Epoch [18/100], Step [600/600], Loss: 0.1266\n",
      "Epoch [19/100], Step [100/600], Loss: 0.1213\n",
      "Epoch [19/100], Step [200/600], Loss: 0.1229\n",
      "Epoch [19/100], Step [300/600], Loss: 0.1250\n",
      "Epoch [19/100], Step [400/600], Loss: 0.1275\n",
      "Epoch [19/100], Step [500/600], Loss: 0.1222\n",
      "Epoch [19/100], Step [600/600], Loss: 0.1253\n",
      "Epoch [20/100], Step [100/600], Loss: 0.1286\n",
      "Epoch [20/100], Step [200/600], Loss: 0.1269\n",
      "Epoch [20/100], Step [300/600], Loss: 0.1234\n",
      "Epoch [20/100], Step [400/600], Loss: 0.1267\n",
      "Epoch [20/100], Step [500/600], Loss: 0.1230\n",
      "Epoch [20/100], Step [600/600], Loss: 0.1234\n",
      "Epoch [21/100], Step [100/600], Loss: 0.1251\n",
      "Epoch [21/100], Step [200/600], Loss: 0.1243\n",
      "Epoch [21/100], Step [300/600], Loss: 0.1214\n",
      "Epoch [21/100], Step [400/600], Loss: 0.1300\n",
      "Epoch [21/100], Step [500/600], Loss: 0.1243\n",
      "Epoch [21/100], Step [600/600], Loss: 0.1260\n",
      "Epoch [22/100], Step [100/600], Loss: 0.1266\n",
      "Epoch [22/100], Step [200/600], Loss: 0.1237\n",
      "Epoch [22/100], Step [300/600], Loss: 0.1315\n",
      "Epoch [22/100], Step [400/600], Loss: 0.1259\n",
      "Epoch [22/100], Step [500/600], Loss: 0.1280\n",
      "Epoch [22/100], Step [600/600], Loss: 0.1265\n",
      "Epoch [23/100], Step [100/600], Loss: 0.1262\n",
      "Epoch [23/100], Step [200/600], Loss: 0.1243\n",
      "Epoch [23/100], Step [300/600], Loss: 0.1286\n",
      "Epoch [23/100], Step [400/600], Loss: 0.1245\n",
      "Epoch [23/100], Step [500/600], Loss: 0.1224\n",
      "Epoch [23/100], Step [600/600], Loss: 0.1249\n",
      "Epoch [24/100], Step [100/600], Loss: 0.1211\n",
      "Epoch [24/100], Step [200/600], Loss: 0.1248\n",
      "Epoch [24/100], Step [300/600], Loss: 0.1270\n",
      "Epoch [24/100], Step [400/600], Loss: 0.1207\n",
      "Epoch [24/100], Step [500/600], Loss: 0.1260\n",
      "Epoch [24/100], Step [600/600], Loss: 0.1243\n",
      "Epoch [25/100], Step [100/600], Loss: 0.1184\n",
      "Epoch [25/100], Step [200/600], Loss: 0.1256\n",
      "Epoch [25/100], Step [300/600], Loss: 0.1292\n",
      "Epoch [25/100], Step [400/600], Loss: 0.1263\n",
      "Epoch [25/100], Step [500/600], Loss: 0.1206\n",
      "Epoch [25/100], Step [600/600], Loss: 0.1251\n",
      "Epoch [26/100], Step [100/600], Loss: 0.1234\n",
      "Epoch [26/100], Step [200/600], Loss: 0.1257\n",
      "Epoch [26/100], Step [300/600], Loss: 0.1217\n",
      "Epoch [26/100], Step [400/600], Loss: 0.1253\n",
      "Epoch [26/100], Step [500/600], Loss: 0.1298\n",
      "Epoch [26/100], Step [600/600], Loss: 0.1230\n",
      "Epoch [27/100], Step [100/600], Loss: 0.1276\n",
      "Epoch [27/100], Step [200/600], Loss: 0.1230\n",
      "Epoch [27/100], Step [300/600], Loss: 0.1271\n",
      "Epoch [27/100], Step [400/600], Loss: 0.1205\n",
      "Epoch [27/100], Step [500/600], Loss: 0.1301\n",
      "Epoch [27/100], Step [600/600], Loss: 0.1291\n",
      "Epoch [28/100], Step [100/600], Loss: 0.1219\n",
      "Epoch [28/100], Step [200/600], Loss: 0.1282\n",
      "Epoch [28/100], Step [300/600], Loss: 0.1296\n",
      "Epoch [28/100], Step [400/600], Loss: 0.1313\n",
      "Epoch [28/100], Step [500/600], Loss: 0.1337\n",
      "Epoch [28/100], Step [600/600], Loss: 0.1231\n",
      "Epoch [29/100], Step [100/600], Loss: 0.1245\n",
      "Epoch [29/100], Step [200/600], Loss: 0.1274\n",
      "Epoch [29/100], Step [300/600], Loss: 0.1243\n",
      "Epoch [29/100], Step [400/600], Loss: 0.1282\n",
      "Epoch [29/100], Step [500/600], Loss: 0.1282\n",
      "Epoch [29/100], Step [600/600], Loss: 0.1266\n",
      "Epoch [30/100], Step [100/600], Loss: 0.1237\n",
      "Epoch [30/100], Step [200/600], Loss: 0.1257\n",
      "Epoch [30/100], Step [300/600], Loss: 0.1329\n",
      "Epoch [30/100], Step [400/600], Loss: 0.1286\n",
      "Epoch [30/100], Step [500/600], Loss: 0.1267\n",
      "Epoch [30/100], Step [600/600], Loss: 0.1295\n",
      "Epoch [31/100], Step [100/600], Loss: 0.1267\n",
      "Epoch [31/100], Step [200/600], Loss: 0.1284\n",
      "Epoch [31/100], Step [300/600], Loss: 0.1269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Step [400/600], Loss: 0.1285\n",
      "Epoch [31/100], Step [500/600], Loss: 0.1309\n",
      "Epoch [31/100], Step [600/600], Loss: 0.1283\n",
      "Epoch [32/100], Step [100/600], Loss: 0.1240\n",
      "Epoch [32/100], Step [200/600], Loss: 0.1288\n",
      "Epoch [32/100], Step [300/600], Loss: 0.1223\n",
      "Epoch [32/100], Step [400/600], Loss: 0.1283\n",
      "Epoch [32/100], Step [500/600], Loss: 0.1268\n",
      "Epoch [32/100], Step [600/600], Loss: 0.1250\n",
      "Epoch [33/100], Step [100/600], Loss: 0.1247\n",
      "Epoch [33/100], Step [200/600], Loss: 0.1260\n",
      "Epoch [33/100], Step [300/600], Loss: 0.1270\n",
      "Epoch [33/100], Step [400/600], Loss: 0.1255\n",
      "Epoch [33/100], Step [500/600], Loss: 0.1289\n",
      "Epoch [33/100], Step [600/600], Loss: 0.1309\n",
      "Epoch [34/100], Step [100/600], Loss: 0.1223\n",
      "Epoch [34/100], Step [200/600], Loss: 0.1255\n",
      "Epoch [34/100], Step [300/600], Loss: 0.1248\n",
      "Epoch [34/100], Step [400/600], Loss: 0.1267\n",
      "Epoch [34/100], Step [500/600], Loss: 0.1263\n",
      "Epoch [34/100], Step [600/600], Loss: 0.1306\n",
      "Epoch [35/100], Step [100/600], Loss: 0.1250\n",
      "Epoch [35/100], Step [200/600], Loss: 0.1304\n",
      "Epoch [35/100], Step [300/600], Loss: 0.1303\n",
      "Epoch [35/100], Step [400/600], Loss: 0.1309\n",
      "Epoch [35/100], Step [500/600], Loss: 0.1262\n",
      "Epoch [35/100], Step [600/600], Loss: 0.1297\n",
      "Epoch [36/100], Step [100/600], Loss: 0.1305\n",
      "Epoch [36/100], Step [200/600], Loss: 0.1276\n",
      "Epoch [36/100], Step [300/600], Loss: 0.1301\n",
      "Epoch [36/100], Step [400/600], Loss: 0.1272\n",
      "Epoch [36/100], Step [500/600], Loss: 0.1309\n",
      "Epoch [36/100], Step [600/600], Loss: 0.1259\n",
      "Epoch [37/100], Step [100/600], Loss: 0.1287\n",
      "Epoch [37/100], Step [200/600], Loss: 0.1357\n",
      "Epoch [37/100], Step [300/600], Loss: 0.1321\n",
      "Epoch [37/100], Step [400/600], Loss: 0.1287\n",
      "Epoch [37/100], Step [500/600], Loss: 0.1251\n",
      "Epoch [37/100], Step [600/600], Loss: 0.1320\n",
      "Epoch [38/100], Step [100/600], Loss: 0.1267\n",
      "Epoch [38/100], Step [200/600], Loss: 0.1226\n",
      "Epoch [38/100], Step [300/600], Loss: 0.1298\n",
      "Epoch [38/100], Step [400/600], Loss: 0.1314\n",
      "Epoch [38/100], Step [500/600], Loss: 0.1259\n",
      "Epoch [38/100], Step [600/600], Loss: 0.1248\n",
      "Epoch [39/100], Step [100/600], Loss: 0.1248\n",
      "Epoch [39/100], Step [200/600], Loss: 0.1313\n",
      "Epoch [39/100], Step [300/600], Loss: 0.1296\n",
      "Epoch [39/100], Step [400/600], Loss: 0.1320\n",
      "Epoch [39/100], Step [500/600], Loss: 0.1311\n",
      "Epoch [39/100], Step [600/600], Loss: 0.1285\n",
      "Epoch [40/100], Step [100/600], Loss: 0.1343\n",
      "Epoch [40/100], Step [200/600], Loss: 0.1302\n",
      "Epoch [40/100], Step [300/600], Loss: 0.1308\n",
      "Epoch [40/100], Step [400/600], Loss: 0.1283\n",
      "Epoch [40/100], Step [500/600], Loss: 0.1304\n",
      "Epoch [40/100], Step [600/600], Loss: 0.1281\n",
      "Epoch [41/100], Step [100/600], Loss: 0.1276\n",
      "Epoch [41/100], Step [200/600], Loss: 0.1290\n",
      "Epoch [41/100], Step [300/600], Loss: 0.1300\n",
      "Epoch [41/100], Step [400/600], Loss: 0.1276\n",
      "Epoch [41/100], Step [500/600], Loss: 0.1313\n",
      "Epoch [41/100], Step [600/600], Loss: 0.1286\n",
      "Epoch [42/100], Step [100/600], Loss: 0.1270\n",
      "Epoch [42/100], Step [200/600], Loss: 0.1275\n",
      "Epoch [42/100], Step [300/600], Loss: 0.1286\n",
      "Epoch [42/100], Step [400/600], Loss: 0.1301\n",
      "Epoch [42/100], Step [500/600], Loss: 0.1265\n",
      "Epoch [42/100], Step [600/600], Loss: 0.1302\n",
      "Epoch [43/100], Step [100/600], Loss: 0.1270\n",
      "Epoch [43/100], Step [200/600], Loss: 0.1341\n",
      "Epoch [43/100], Step [300/600], Loss: 0.1271\n",
      "Epoch [43/100], Step [400/600], Loss: 0.1307\n",
      "Epoch [43/100], Step [500/600], Loss: 0.1330\n",
      "Epoch [43/100], Step [600/600], Loss: 0.1265\n",
      "Epoch [44/100], Step [100/600], Loss: 0.1282\n",
      "Epoch [44/100], Step [200/600], Loss: 0.1283\n",
      "Epoch [44/100], Step [300/600], Loss: 0.1324\n",
      "Epoch [44/100], Step [400/600], Loss: 0.1261\n",
      "Epoch [44/100], Step [500/600], Loss: 0.1285\n",
      "Epoch [44/100], Step [600/600], Loss: 0.1326\n",
      "Epoch [45/100], Step [100/600], Loss: 0.1266\n",
      "Epoch [45/100], Step [200/600], Loss: 0.1301\n",
      "Epoch [45/100], Step [300/600], Loss: 0.1324\n",
      "Epoch [45/100], Step [400/600], Loss: 0.1256\n",
      "Epoch [45/100], Step [500/600], Loss: 0.1308\n",
      "Epoch [45/100], Step [600/600], Loss: 0.1309\n",
      "Epoch [46/100], Step [100/600], Loss: 0.1289\n",
      "Epoch [46/100], Step [200/600], Loss: 0.1325\n",
      "Epoch [46/100], Step [300/600], Loss: 0.1249\n",
      "Epoch [46/100], Step [400/600], Loss: 0.1335\n",
      "Epoch [46/100], Step [500/600], Loss: 0.1322\n",
      "Epoch [46/100], Step [600/600], Loss: 0.1286\n",
      "Epoch [47/100], Step [100/600], Loss: 0.1302\n",
      "Epoch [47/100], Step [200/600], Loss: 0.1252\n",
      "Epoch [47/100], Step [300/600], Loss: 0.1314\n",
      "Epoch [47/100], Step [400/600], Loss: 0.1302\n",
      "Epoch [47/100], Step [500/600], Loss: 0.1269\n",
      "Epoch [47/100], Step [600/600], Loss: 0.1316\n",
      "Epoch [48/100], Step [100/600], Loss: 0.1333\n",
      "Epoch [48/100], Step [200/600], Loss: 0.1318\n",
      "Epoch [48/100], Step [300/600], Loss: 0.1335\n",
      "Epoch [48/100], Step [400/600], Loss: 0.1298\n",
      "Epoch [48/100], Step [500/600], Loss: 0.1299\n",
      "Epoch [48/100], Step [600/600], Loss: 0.1290\n",
      "Epoch [49/100], Step [100/600], Loss: 0.1284\n",
      "Epoch [49/100], Step [200/600], Loss: 0.1307\n",
      "Epoch [49/100], Step [300/600], Loss: 0.1302\n",
      "Epoch [49/100], Step [400/600], Loss: 0.1255\n",
      "Epoch [49/100], Step [500/600], Loss: 0.1289\n",
      "Epoch [49/100], Step [600/600], Loss: 0.1281\n",
      "Epoch [50/100], Step [100/600], Loss: 0.1317\n",
      "Epoch [50/100], Step [200/600], Loss: 0.1306\n",
      "Epoch [50/100], Step [300/600], Loss: 0.1312\n",
      "Epoch [50/100], Step [400/600], Loss: 0.1333\n",
      "Epoch [50/100], Step [500/600], Loss: 0.1349\n",
      "Epoch [50/100], Step [600/600], Loss: 0.1247\n",
      "Epoch [51/100], Step [100/600], Loss: 0.1335\n",
      "Epoch [51/100], Step [200/600], Loss: 0.1274\n",
      "Epoch [51/100], Step [300/600], Loss: 0.1324\n",
      "Epoch [51/100], Step [400/600], Loss: 0.1315\n",
      "Epoch [51/100], Step [500/600], Loss: 0.1281\n",
      "Epoch [51/100], Step [600/600], Loss: 0.1372\n",
      "Epoch [52/100], Step [100/600], Loss: 0.1315\n",
      "Epoch [52/100], Step [200/600], Loss: 0.1297\n",
      "Epoch [52/100], Step [300/600], Loss: 0.1356\n",
      "Epoch [52/100], Step [400/600], Loss: 0.1276\n",
      "Epoch [52/100], Step [500/600], Loss: 0.1297\n",
      "Epoch [52/100], Step [600/600], Loss: 0.1389\n",
      "Epoch [53/100], Step [100/600], Loss: 0.1256\n",
      "Epoch [53/100], Step [200/600], Loss: 0.1311\n",
      "Epoch [53/100], Step [300/600], Loss: 0.1312\n",
      "Epoch [53/100], Step [400/600], Loss: 0.1275\n",
      "Epoch [53/100], Step [500/600], Loss: 0.1311\n",
      "Epoch [53/100], Step [600/600], Loss: 0.1283\n",
      "Epoch [54/100], Step [100/600], Loss: 0.1306\n",
      "Epoch [54/100], Step [200/600], Loss: 0.1291\n",
      "Epoch [54/100], Step [300/600], Loss: 0.1344\n",
      "Epoch [54/100], Step [400/600], Loss: 0.1336\n",
      "Epoch [54/100], Step [500/600], Loss: 0.1311\n",
      "Epoch [54/100], Step [600/600], Loss: 0.1340\n",
      "Epoch [55/100], Step [100/600], Loss: 0.1293\n",
      "Epoch [55/100], Step [200/600], Loss: 0.1314\n",
      "Epoch [55/100], Step [300/600], Loss: 0.1333\n",
      "Epoch [55/100], Step [400/600], Loss: 0.1318\n",
      "Epoch [55/100], Step [500/600], Loss: 0.1309\n",
      "Epoch [55/100], Step [600/600], Loss: 0.1314\n",
      "Epoch [56/100], Step [100/600], Loss: 0.1275\n",
      "Epoch [56/100], Step [200/600], Loss: 0.1325\n",
      "Epoch [56/100], Step [300/600], Loss: 0.1266\n",
      "Epoch [56/100], Step [400/600], Loss: 0.1290\n",
      "Epoch [56/100], Step [500/600], Loss: 0.1351\n",
      "Epoch [56/100], Step [600/600], Loss: 0.1339\n",
      "Epoch [57/100], Step [100/600], Loss: 0.1257\n",
      "Epoch [57/100], Step [200/600], Loss: 0.1300\n",
      "Epoch [57/100], Step [300/600], Loss: 0.1283\n",
      "Epoch [57/100], Step [400/600], Loss: 0.1302\n",
      "Epoch [57/100], Step [500/600], Loss: 0.1337\n",
      "Epoch [57/100], Step [600/600], Loss: 0.1356\n",
      "Epoch [58/100], Step [100/600], Loss: 0.1268\n",
      "Epoch [58/100], Step [200/600], Loss: 0.1279\n",
      "Epoch [58/100], Step [300/600], Loss: 0.1335\n",
      "Epoch [58/100], Step [400/600], Loss: 0.1310\n",
      "Epoch [58/100], Step [500/600], Loss: 0.1326\n",
      "Epoch [58/100], Step [600/600], Loss: 0.1307\n",
      "Epoch [59/100], Step [100/600], Loss: 0.1313\n",
      "Epoch [59/100], Step [200/600], Loss: 0.1316\n",
      "Epoch [59/100], Step [300/600], Loss: 0.1263\n",
      "Epoch [59/100], Step [400/600], Loss: 0.1329\n",
      "Epoch [59/100], Step [500/600], Loss: 0.1348\n",
      "Epoch [59/100], Step [600/600], Loss: 0.1285\n",
      "Epoch [60/100], Step [100/600], Loss: 0.1315\n",
      "Epoch [60/100], Step [200/600], Loss: 0.1323\n",
      "Epoch [60/100], Step [300/600], Loss: 0.1335\n",
      "Epoch [60/100], Step [400/600], Loss: 0.1316\n",
      "Epoch [60/100], Step [500/600], Loss: 0.1302\n",
      "Epoch [60/100], Step [600/600], Loss: 0.1303\n",
      "Epoch [61/100], Step [100/600], Loss: 0.1311\n",
      "Epoch [61/100], Step [200/600], Loss: 0.1299\n",
      "Epoch [61/100], Step [300/600], Loss: 0.1288\n",
      "Epoch [61/100], Step [400/600], Loss: 0.1326\n",
      "Epoch [61/100], Step [500/600], Loss: 0.1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100], Step [600/600], Loss: 0.1350\n",
      "Epoch [62/100], Step [100/600], Loss: 0.1293\n",
      "Epoch [62/100], Step [200/600], Loss: 0.1345\n",
      "Epoch [62/100], Step [300/600], Loss: 0.1334\n",
      "Epoch [62/100], Step [400/600], Loss: 0.1295\n",
      "Epoch [62/100], Step [500/600], Loss: 0.1292\n",
      "Epoch [62/100], Step [600/600], Loss: 0.1327\n",
      "Epoch [63/100], Step [100/600], Loss: 0.1292\n",
      "Epoch [63/100], Step [200/600], Loss: 0.1299\n",
      "Epoch [63/100], Step [300/600], Loss: 0.1337\n",
      "Epoch [63/100], Step [400/600], Loss: 0.1343\n",
      "Epoch [63/100], Step [500/600], Loss: 0.1355\n",
      "Epoch [63/100], Step [600/600], Loss: 0.1357\n",
      "Epoch [64/100], Step [100/600], Loss: 0.1335\n",
      "Epoch [64/100], Step [200/600], Loss: 0.1352\n",
      "Epoch [64/100], Step [300/600], Loss: 0.1282\n",
      "Epoch [64/100], Step [400/600], Loss: 0.1331\n",
      "Epoch [64/100], Step [500/600], Loss: 0.1321\n",
      "Epoch [64/100], Step [600/600], Loss: 0.1311\n",
      "Epoch [65/100], Step [100/600], Loss: 0.1305\n",
      "Epoch [65/100], Step [200/600], Loss: 0.1313\n",
      "Epoch [65/100], Step [300/600], Loss: 0.1296\n",
      "Epoch [65/100], Step [400/600], Loss: 0.1316\n",
      "Epoch [65/100], Step [500/600], Loss: 0.1310\n",
      "Epoch [65/100], Step [600/600], Loss: 0.1284\n",
      "Epoch [66/100], Step [100/600], Loss: 0.1273\n",
      "Epoch [66/100], Step [200/600], Loss: 0.1339\n",
      "Epoch [66/100], Step [300/600], Loss: 0.1340\n",
      "Epoch [66/100], Step [400/600], Loss: 0.1266\n",
      "Epoch [66/100], Step [500/600], Loss: 0.1349\n",
      "Epoch [66/100], Step [600/600], Loss: 0.1349\n",
      "Epoch [67/100], Step [100/600], Loss: 0.1345\n",
      "Epoch [67/100], Step [200/600], Loss: 0.1361\n",
      "Epoch [67/100], Step [300/600], Loss: 0.1307\n",
      "Epoch [67/100], Step [400/600], Loss: 0.1286\n",
      "Epoch [67/100], Step [500/600], Loss: 0.1301\n",
      "Epoch [67/100], Step [600/600], Loss: 0.1300\n",
      "Epoch [68/100], Step [100/600], Loss: 0.1366\n",
      "Epoch [68/100], Step [200/600], Loss: 0.1305\n",
      "Epoch [68/100], Step [300/600], Loss: 0.1289\n",
      "Epoch [68/100], Step [400/600], Loss: 0.1328\n",
      "Epoch [68/100], Step [500/600], Loss: 0.1338\n",
      "Epoch [68/100], Step [600/600], Loss: 0.1342\n",
      "Epoch [69/100], Step [100/600], Loss: 0.1293\n",
      "Epoch [69/100], Step [200/600], Loss: 0.1354\n",
      "Epoch [69/100], Step [300/600], Loss: 0.1322\n",
      "Epoch [69/100], Step [400/600], Loss: 0.1300\n",
      "Epoch [69/100], Step [500/600], Loss: 0.1320\n",
      "Epoch [69/100], Step [600/600], Loss: 0.1323\n",
      "Epoch [70/100], Step [100/600], Loss: 0.1305\n",
      "Epoch [70/100], Step [200/600], Loss: 0.1386\n",
      "Epoch [70/100], Step [300/600], Loss: 0.1288\n",
      "Epoch [70/100], Step [400/600], Loss: 0.1297\n",
      "Epoch [70/100], Step [500/600], Loss: 0.1388\n",
      "Epoch [70/100], Step [600/600], Loss: 0.1329\n",
      "Epoch [71/100], Step [100/600], Loss: 0.1316\n",
      "Epoch [71/100], Step [200/600], Loss: 0.1304\n",
      "Epoch [71/100], Step [300/600], Loss: 0.1299\n",
      "Epoch [71/100], Step [400/600], Loss: 0.1340\n",
      "Epoch [71/100], Step [500/600], Loss: 0.1332\n",
      "Epoch [71/100], Step [600/600], Loss: 0.1320\n",
      "Epoch [72/100], Step [100/600], Loss: 0.1358\n",
      "Epoch [72/100], Step [200/600], Loss: 0.1307\n",
      "Epoch [72/100], Step [300/600], Loss: 0.1305\n",
      "Epoch [72/100], Step [400/600], Loss: 0.1308\n",
      "Epoch [72/100], Step [500/600], Loss: 0.1333\n",
      "Epoch [72/100], Step [600/600], Loss: 0.1289\n",
      "Epoch [73/100], Step [100/600], Loss: 0.1357\n",
      "Epoch [73/100], Step [200/600], Loss: 0.1301\n",
      "Epoch [73/100], Step [300/600], Loss: 0.1341\n",
      "Epoch [73/100], Step [400/600], Loss: 0.1358\n",
      "Epoch [73/100], Step [500/600], Loss: 0.1292\n",
      "Epoch [73/100], Step [600/600], Loss: 0.1276\n",
      "Epoch [74/100], Step [100/600], Loss: 0.1288\n",
      "Epoch [74/100], Step [200/600], Loss: 0.1400\n",
      "Epoch [74/100], Step [300/600], Loss: 0.1321\n",
      "Epoch [74/100], Step [400/600], Loss: 0.1348\n",
      "Epoch [74/100], Step [500/600], Loss: 0.1366\n",
      "Epoch [74/100], Step [600/600], Loss: 0.1313\n",
      "Epoch [75/100], Step [100/600], Loss: 0.1328\n",
      "Epoch [75/100], Step [200/600], Loss: 0.1298\n",
      "Epoch [75/100], Step [300/600], Loss: 0.1285\n",
      "Epoch [75/100], Step [400/600], Loss: 0.1283\n",
      "Epoch [75/100], Step [500/600], Loss: 0.1353\n",
      "Epoch [75/100], Step [600/600], Loss: 0.1307\n",
      "Epoch [76/100], Step [100/600], Loss: 0.1260\n",
      "Epoch [76/100], Step [200/600], Loss: 0.1275\n",
      "Epoch [76/100], Step [300/600], Loss: 0.1290\n",
      "Epoch [76/100], Step [400/600], Loss: 0.1306\n",
      "Epoch [76/100], Step [500/600], Loss: 0.1290\n",
      "Epoch [76/100], Step [600/600], Loss: 0.1284\n",
      "Epoch [77/100], Step [100/600], Loss: 0.1331\n",
      "Epoch [77/100], Step [200/600], Loss: 0.1375\n",
      "Epoch [77/100], Step [300/600], Loss: 0.1388\n",
      "Epoch [77/100], Step [400/600], Loss: 0.1359\n",
      "Epoch [77/100], Step [500/600], Loss: 0.1350\n",
      "Epoch [77/100], Step [600/600], Loss: 0.1357\n",
      "Epoch [78/100], Step [100/600], Loss: 0.1343\n",
      "Epoch [78/100], Step [200/600], Loss: 0.1296\n",
      "Epoch [78/100], Step [300/600], Loss: 0.1305\n",
      "Epoch [78/100], Step [400/600], Loss: 0.1382\n",
      "Epoch [78/100], Step [500/600], Loss: 0.1339\n",
      "Epoch [78/100], Step [600/600], Loss: 0.1291\n",
      "Epoch [79/100], Step [100/600], Loss: 0.1252\n",
      "Epoch [79/100], Step [200/600], Loss: 0.1324\n",
      "Epoch [79/100], Step [300/600], Loss: 0.1304\n",
      "Epoch [79/100], Step [400/600], Loss: 0.1349\n",
      "Epoch [79/100], Step [500/600], Loss: 0.1354\n",
      "Epoch [79/100], Step [600/600], Loss: 0.1346\n",
      "Epoch [80/100], Step [100/600], Loss: 0.1332\n",
      "Epoch [80/100], Step [200/600], Loss: 0.1317\n",
      "Epoch [80/100], Step [300/600], Loss: 0.1388\n",
      "Epoch [80/100], Step [400/600], Loss: 0.1334\n",
      "Epoch [80/100], Step [500/600], Loss: 0.1312\n",
      "Epoch [80/100], Step [600/600], Loss: 0.1343\n",
      "Epoch [81/100], Step [100/600], Loss: 0.1301\n",
      "Epoch [81/100], Step [200/600], Loss: 0.1333\n",
      "Epoch [81/100], Step [300/600], Loss: 0.1304\n",
      "Epoch [81/100], Step [400/600], Loss: 0.1316\n",
      "Epoch [81/100], Step [500/600], Loss: 0.1334\n",
      "Epoch [81/100], Step [600/600], Loss: 0.1334\n",
      "Epoch [82/100], Step [100/600], Loss: 0.1369\n",
      "Epoch [82/100], Step [200/600], Loss: 0.1330\n",
      "Epoch [82/100], Step [300/600], Loss: 0.1273\n",
      "Epoch [82/100], Step [400/600], Loss: 0.1296\n",
      "Epoch [82/100], Step [500/600], Loss: 0.1341\n",
      "Epoch [82/100], Step [600/600], Loss: 0.1348\n",
      "Epoch [83/100], Step [100/600], Loss: 0.1348\n",
      "Epoch [83/100], Step [200/600], Loss: 0.1328\n",
      "Epoch [83/100], Step [300/600], Loss: 0.1321\n",
      "Epoch [83/100], Step [400/600], Loss: 0.1292\n",
      "Epoch [83/100], Step [500/600], Loss: 0.1332\n",
      "Epoch [83/100], Step [600/600], Loss: 0.1338\n",
      "Epoch [84/100], Step [100/600], Loss: 0.1332\n",
      "Epoch [84/100], Step [200/600], Loss: 0.1291\n",
      "Epoch [84/100], Step [300/600], Loss: 0.1362\n",
      "Epoch [84/100], Step [400/600], Loss: 0.1329\n",
      "Epoch [84/100], Step [500/600], Loss: 0.1289\n",
      "Epoch [84/100], Step [600/600], Loss: 0.1350\n",
      "Epoch [85/100], Step [100/600], Loss: 0.1328\n",
      "Epoch [85/100], Step [200/600], Loss: 0.1327\n",
      "Epoch [85/100], Step [300/600], Loss: 0.1332\n",
      "Epoch [85/100], Step [400/600], Loss: 0.1308\n",
      "Epoch [85/100], Step [500/600], Loss: 0.1290\n",
      "Epoch [85/100], Step [600/600], Loss: 0.1367\n",
      "Epoch [86/100], Step [100/600], Loss: 0.1345\n",
      "Epoch [86/100], Step [200/600], Loss: 0.1341\n",
      "Epoch [86/100], Step [300/600], Loss: 0.1292\n",
      "Epoch [86/100], Step [400/600], Loss: 0.1286\n",
      "Epoch [86/100], Step [500/600], Loss: 0.1350\n",
      "Epoch [86/100], Step [600/600], Loss: 0.1304\n",
      "Epoch [87/100], Step [100/600], Loss: 0.1413\n",
      "Epoch [87/100], Step [200/600], Loss: 0.1308\n",
      "Epoch [87/100], Step [300/600], Loss: 0.1286\n",
      "Epoch [87/100], Step [400/600], Loss: 0.1355\n",
      "Epoch [87/100], Step [500/600], Loss: 0.1324\n",
      "Epoch [87/100], Step [600/600], Loss: 0.1312\n",
      "Epoch [88/100], Step [100/600], Loss: 0.1289\n",
      "Epoch [88/100], Step [200/600], Loss: 0.1344\n",
      "Epoch [88/100], Step [300/600], Loss: 0.1299\n",
      "Epoch [88/100], Step [400/600], Loss: 0.1311\n",
      "Epoch [88/100], Step [500/600], Loss: 0.1346\n",
      "Epoch [88/100], Step [600/600], Loss: 0.1250\n",
      "Epoch [89/100], Step [100/600], Loss: 0.1308\n",
      "Epoch [89/100], Step [200/600], Loss: 0.1341\n",
      "Epoch [89/100], Step [300/600], Loss: 0.1266\n",
      "Epoch [89/100], Step [400/600], Loss: 0.1381\n",
      "Epoch [89/100], Step [500/600], Loss: 0.1384\n",
      "Epoch [89/100], Step [600/600], Loss: 0.1302\n",
      "Epoch [90/100], Step [100/600], Loss: 0.1435\n",
      "Epoch [90/100], Step [200/600], Loss: 0.1352\n",
      "Epoch [90/100], Step [300/600], Loss: 0.1350\n",
      "Epoch [90/100], Step [400/600], Loss: 0.1366\n",
      "Epoch [90/100], Step [500/600], Loss: 0.1304\n",
      "Epoch [90/100], Step [600/600], Loss: 0.1325\n",
      "Epoch [91/100], Step [100/600], Loss: 0.1310\n",
      "Epoch [91/100], Step [200/600], Loss: 0.1332\n",
      "Epoch [91/100], Step [300/600], Loss: 0.1334\n",
      "Epoch [91/100], Step [400/600], Loss: 0.1349\n",
      "Epoch [91/100], Step [500/600], Loss: 0.1293\n",
      "Epoch [91/100], Step [600/600], Loss: 0.1346\n",
      "Epoch [92/100], Step [100/600], Loss: 0.1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Step [200/600], Loss: 0.1311\n",
      "Epoch [92/100], Step [300/600], Loss: 0.1283\n",
      "Epoch [92/100], Step [400/600], Loss: 0.1339\n",
      "Epoch [92/100], Step [500/600], Loss: 0.1343\n",
      "Epoch [92/100], Step [600/600], Loss: 0.1300\n",
      "Epoch [93/100], Step [100/600], Loss: 0.1367\n",
      "Epoch [93/100], Step [200/600], Loss: 0.1303\n",
      "Epoch [93/100], Step [300/600], Loss: 0.1291\n",
      "Epoch [93/100], Step [400/600], Loss: 0.1299\n",
      "Epoch [93/100], Step [500/600], Loss: 0.1326\n",
      "Epoch [93/100], Step [600/600], Loss: 0.1371\n",
      "Epoch [94/100], Step [100/600], Loss: 0.1318\n",
      "Epoch [94/100], Step [200/600], Loss: 0.1267\n",
      "Epoch [94/100], Step [300/600], Loss: 0.1305\n",
      "Epoch [94/100], Step [400/600], Loss: 0.1371\n",
      "Epoch [94/100], Step [500/600], Loss: 0.1264\n",
      "Epoch [94/100], Step [600/600], Loss: 0.1279\n",
      "Epoch [95/100], Step [100/600], Loss: 0.1354\n",
      "Epoch [95/100], Step [200/600], Loss: 0.1295\n",
      "Epoch [95/100], Step [300/600], Loss: 0.1331\n",
      "Epoch [95/100], Step [400/600], Loss: 0.1371\n",
      "Epoch [95/100], Step [500/600], Loss: 0.1309\n",
      "Epoch [95/100], Step [600/600], Loss: 0.1277\n",
      "Epoch [96/100], Step [100/600], Loss: 0.1344\n",
      "Epoch [96/100], Step [200/600], Loss: 0.1314\n",
      "Epoch [96/100], Step [300/600], Loss: 0.1384\n",
      "Epoch [96/100], Step [400/600], Loss: 0.1326\n",
      "Epoch [96/100], Step [500/600], Loss: 0.1324\n",
      "Epoch [96/100], Step [600/600], Loss: 0.1357\n",
      "Epoch [97/100], Step [100/600], Loss: 0.1255\n",
      "Epoch [97/100], Step [200/600], Loss: 0.1308\n",
      "Epoch [97/100], Step [300/600], Loss: 0.1290\n",
      "Epoch [97/100], Step [400/600], Loss: 0.1340\n",
      "Epoch [97/100], Step [500/600], Loss: 0.1342\n",
      "Epoch [97/100], Step [600/600], Loss: 0.1294\n",
      "Epoch [98/100], Step [100/600], Loss: 0.1281\n",
      "Epoch [98/100], Step [200/600], Loss: 0.1323\n",
      "Epoch [98/100], Step [300/600], Loss: 0.1351\n",
      "Epoch [98/100], Step [400/600], Loss: 0.1378\n",
      "Epoch [98/100], Step [500/600], Loss: 0.1304\n",
      "Epoch [98/100], Step [600/600], Loss: 0.1358\n",
      "Epoch [99/100], Step [100/600], Loss: 0.1350\n",
      "Epoch [99/100], Step [200/600], Loss: 0.1313\n",
      "Epoch [99/100], Step [300/600], Loss: 0.1330\n",
      "Epoch [99/100], Step [400/600], Loss: 0.1330\n",
      "Epoch [99/100], Step [500/600], Loss: 0.1318\n",
      "Epoch [99/100], Step [600/600], Loss: 0.1279\n",
      "Epoch [100/100], Step [100/600], Loss: 0.1317\n",
      "Epoch [100/100], Step [200/600], Loss: 0.1348\n",
      "Epoch [100/100], Step [300/600], Loss: 0.1311\n",
      "Epoch [100/100], Step [400/600], Loss: 0.1362\n",
      "Epoch [100/100], Step [500/600], Loss: 0.1351\n",
      "Epoch [100/100], Step [600/600], Loss: 0.1321\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = images.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hidden_op,outputs = model(images)\n",
    "        en_weights = model.state_dict()['encoder.0.weight']\n",
    "        loss = criterion.forward(outputs, images,en_weights,hidden_op)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if((i+1) % 100 == 0):\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_op(n_images,orig_images,decoded_images,image_width):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=n_images, \n",
    "                         sharex=True, sharey=True, figsize=(20, 2.5))\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    # hide tick and tick label of the big axes\n",
    "    plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "    plt.grid(False)\n",
    "    plt.ylabel(\"Decoded (Bottom)   |   Original(Top)\")\n",
    "    orig_images = orig_images[:n_images]\n",
    "    decoded_images = decoded_images[:n_images]\n",
    "    \n",
    "    \n",
    "    for i in range(n_images):\n",
    "        for ax, img in zip(axes, [orig_images, decoded_images]):\n",
    "            ax[i].imshow(img[i].detach().reshape((image_width, image_width)),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images,_ = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,test_output = model(test_images.view(-1,28*28))\n",
    "test_output =test_output.view(100,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit_tare\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x180 with 17 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validate_op(8,test_images,test_output,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
